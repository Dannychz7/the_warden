====================================================================================
tool_executor.py - Main Agent Controller for "The Warden"
====================================================================================

- Purpose: Manages the execution of various tools (scripts/APIs) via the MCPManager for a cybersecurity system ("The Warden").
- Primary Class: ToolExecutor
- Context: Tightly coupled with an MCP server, which orchestrates tool execution across environments.

# How Long Would This Script Take to Run?
- Startup Time: Instantaneous (it's just class and method definitions).
- Execution Time:
    - Depends entirely on the tool being called via mcp_manager.call_tool().
    - Includes: argument validation, JSON parsing, and tool result processing.
    - Factors affecting runtime:
        - Latency of remote tool execution.
        - Complexity of JSON result processing.
        - Network delays and server response time.

# What Does This Script Do?
- Manages:
    - Available tools across servers.
    - Execution of selected tools. 
    - Argument validation before execution.
    - Result parsing and formatting.
    - Usage statistics tracking and reporting.
    - Enables an LLM or SOC analyst to:
        - Select tools dynamically.
        - Understand what inputs are required.
        - View structured and summarized output.
        - Provides safeguards (error handling, input validation, and fallback responses).

# Why Is It Important to an MCP Server?
- The MCP server is the command center for executing remote procedures (tools).
- This class:
    - Acts as a middleware layer between UI/LLM and backend tools.
    - Abstracts away raw tool details (e.g., tool schema, execution method).
    - Provides auditing and observability through stats and logs.
    - Validates security by enforcing proper argument formats before execution.

# Example Use Case
- Let’s say the tool search_ip_threat_intel is called with {"ip": "8.8.8.8"}:
    - ToolExecutor checks if it exists and validates that ip is a required string.
    - Executes via mcp_manager.call_tool(...).
        - Gets result like:
            {
            "result": {
                "content": [
                {"text": "{\"ip\": \"8.8.8.8\", \"threat_level\": \"low\", \"confidence\": 10}"}
                ]
            }
            }
    - Parses JSON, generates a human summary like:
    - "IP 8.8.8.8: 10% confidence, threat level: low"

"""
====================================================================================
theWarden.py - Main Agent Controller for "The Warden"
====================================================================================

Purpose:
- This is the **main entry point** and control logic for an autonomous SOC analyst 
  system called "The Warden" that uses Qwen3 (an LLM) to automate threat detection,
  analysis, and tool execution via an MCP server.

What It Does:
- Loads and initializes the MCP tool ecosystem.
- Uses a loop to allow the LLM (Qwen3) to think and decide actions step-by-step.
- Executes relevant tools using the ToolExecutor.
- Collects results and sends them back to the LLM for iterative refinement.
- Outputs a final analysis report based on tool outputs.

How Long Does It Take:
- **Initialization**: Fast (server startup + tool loading depends on environment).
- **Analysis Loop**: Up to `max_iterations` (default 5). Total time depends on:
  - Tool execution time (network I/O, threat intel APIs).
  - LLM response latency.
- **Typical Runtime**: Seconds to minutes per query depending on tools used.

Importance to MCP Server:
- Orchestrates the entire autonomous SOC workflow.
- Manages the control loop between:
  - Threat analysis queries
  - Tool execution
  - LLM decision-making
- Acts as the main **brain and conductor** of the SOC agent pipeline.

Key Components and Their Roles:
- `MCPManager` – Manages tool server startup/shutdown and tool inventory.
- `LLMInterface` – Sends context to the LLM and receives next actions.
- `ToolExecutor` – Executes tools with validated arguments and returns structured results.
- `analyze()` – Core logic loop where Qwen3 iteratively makes decisions (up to 5 times).
- `interactive_mode()` – CLI interface that allows live interaction with The Warden.
- `main()` – Entry point that chooses between CLI or script mode (based on `sys.argv`).

Key Functional Steps:
1. Start all MCP servers and fetch available tools.
2. Begin an iterative thinking/acting loop:
   - LLM suggests an action (e.g., call a tool).
   - Tool is executed and results appended to context.
   - Repeats until LLM decides the analysis is complete.
3. Final report generated by LLM based on the full context.

Suggestions for Refinement & Optimization:
- **Async Tool Calls**: Make `execute_tool` async to parallelize multi-step workflows.
- **Configurable Iterations**: Allow user to override `max_iterations` dynamically.
- **Tool Result Caching**: Avoid re-calling the same tool if the context hasn’t changed.
- **Improved Logging**: Replace `print()` with the `logging` module for structured logs.
- **Time Tracking**: Log execution time per iteration for performance monitoring.
- **Analysis Metadata**: Return analysis metadata (duration, tools used, success rate).

Example Query Flow:
User: "Investigate the reputation of IP 45.33.32.156"
- Qwen3 suggests using `check_ip_reputation`
- Tool runs, result stored
- Qwen3 suggests another tool or declares analysis complete
- Final LLM summary generated with findings

====================================================================================

====================================================================================
mcp_server_config.json - Configuration for MCP Tool Servers
====================================================================================
Purpose:
- Defines the configuration for launching and managing multiple **MCP (Modular Command Protocol) tool servers**.
- Each entry describes how to start a specific backend threat intelligence service and what its capabilities are.

What It Does:
- Specifies:
  - Executable command and arguments for launching each tool server.
  - Description of what each server does (used for display in UI/LLM).
  - Transport protocol (e.g., stdio, TCP).
  - Supported capabilities (e.g., "tools").
- Provides client-side configuration (timeouts and retry behavior).

How Long Does It Take:
- This config itself is instantaneous to load.
- However, the **launch time** of each server script (e.g., `abuseIP_mcp_server.py`) depends on:
  - Script complexity
  - Network latency (e.g., when calling external APIs)
  - Cold-start latency (if the tool requires initialization like loading an Elasticsearch index)

Importance to MCP Server:
- Acts as the **central registry** and launch configuration for all tool servers.
- Tells the `MCPManager` which tools to start and how to communicate with them.
- Essential for dynamic tool discovery, transport routing, and unified management of tool processes.

Key Sections and Fields:

1. `"mcpServers"`:
   - **Key**: Unique name of the server (e.g., `"abuseipdb-server"`).
   - **Fields**:
     - `"command"`: Executable (here, all are `python3`).
     - `"args"`: Script path to run.
     - `"description"`: Human-readable summary of what the tool does.
     - `"capabilities"`: Role of the tool (currently all `"tools"`).
     - `"transport"`: How the client communicates with it (currently `"stdio"` — via subprocess).

2. `"clientSettings"`:
   - `"timeout"`: Max time (in ms) to wait for a tool response (default `60000` = 60 sec).
   - `"retryAttempts"`: Number of times to retry a failed tool call before giving up.

MCP Servers Defined:
- `abuseipdb-server`
  - Checks IP reputation via AbuseIPDB API.
- `threatfox-server`
  - Gathers IOCs and intel from ThreatFox (abuse.ch).
- `elastic-server`
  - Interfaces with Elasticsearch SIEM data for threat hunting, document queries, and DSL search.

How It Integrates:
- When `TheWarden` is initialized:
  - `MCPManager` reads this file.
  - Starts each server listed under `mcpServers` using subprocess + stdio.
  - Registers available tools and passes them to `ToolExecutor` + LLM.

Suggestions for Refinement & Optimization:
- **Add health check URLs** for each server (optional for advanced monitoring).
- **Support more transport options** (e.g., sockets or HTTP for remote tools).
- **Allow config grouping/tags** (e.g., "intel", "forensics", "correlation").
- **Expose logging level settings** (per server).
- **Make timeout customizable per tool** (some tools may take longer by design).
- **Add version field** to enforce compatibility between tools and MCPManager.

Example Use Case:
- If a user queries "Check threat level for IP 45.33.32.156":
  - `TheWarden` instructs `ToolExecutor` to call a tool from `abuseipdb-server`.
  - This server is launched based on the `command` and `args` defined here.
  - Result flows back through the same transport, into the LLM.

====================================================================================

====================================================================================
mcp_manager.py - Core MCP Process & Tool Management for TheWarden
====================================================================================

Purpose:
- Launches, manages, and communicates with threat intelligence tools that follow the **MCP (Modular Command Protocol)** via JSON-RPC over `stdio`.
- Provides an abstraction layer between external tool servers (e.g. AbuseIPDB, ThreatFox, Elastic) and TheWarden's ToolExecutor or LLM interface.

What It Does:
- Reads a config JSON (typically `mcp_servers.json`) listing multiple "tool servers".
- For each defined server:
  - Launches it as a subprocess
  - Sends an `initialize` JSON-RPC message
  - Retrieves and caches available tools
- Supports:
  - Tool discovery
  - Tool invocation via `tools/call`
  - Health monitoring
  - Graceful shutdown of servers

How Long Does It Take to Run?
- The manager class itself loads quickly (~100–500ms).
- Actual runtime cost depends on:
  - Number of tool servers defined in config
  - Tool startup delay (`startup_timeout`)
  - Latency of tool APIs (e.g., AbuseIPDB)
- Example: Starting 3 tools that each take ~2s results in ~6s startup time.

Importance to MCP System:
- This file is the **backbone** of the entire MCP infrastructure:
  - Without it, tools won't be discovered or launched.
  - Tool calls would have no target process to handle them.
- Abstracts subprocess management, communication, and error handling from the LLM/ToolExecutor layer.

Key Classes and Their Roles:
1. `MCPServer`:
   - Represents a **single MCP server instance**.
   - Handles:
     - Launching the tool subprocess
     - Sending/receiving JSON-RPC messages
     - Retrieving tool list (`tools/list`)
     - Invoking tools (`tools/call`)
     - Restart/stop/health-check logic
2. `MCPManager`:
   - Orchestrates **multiple `MCPServer` instances**.
   - Provides:
     - Global tool discovery (`get_all_tools`)
     - Tool routing (`call_tool`)
     - Bulk startup/shutdown
     - Centralized server health/status tracking

Features & Notable Behaviors:
- Uses `stdin/stdout` pipes to simulate a persistent connection to each server.
- Tools are only available after successful `initialize` → `tools/list` handshake.
- Timeouts and retry logic respect client-level config (`timeout`, `retryAttempts`).
- Supports forwarding tool calls by name, decoupling LLM from tool internals.

How It Integrates:
- On Warden startup:
  1. `MCPManager(config_file)` is instantiated.
  2. `start_all_servers()` launches each tool backend.
  3. ToolExecutor queries `get_all_tools()` for registered tools.
  4. When a tool is invoked, `call_tool(tool_name, args)` routes to the correct subprocess.

Example Usage Flow:
- User queries: “Search IP `1.2.3.4` across SIEM.”
- ToolExecutor → `MCPManager.call_tool("elastic.search_ip", {"ip": "1.2.3.4"})`
- `MCPManager`:
  - Locates which MCPServer provides `"elastic.search_ip"`
  - Sends `tools/call` request
  - Returns result to LLM

Failover Handling:
- If a tool server crashes:
  - `health_check_all()` detects it.
  - `call_tool()` fails gracefully with context.
- If a process fails during launch:
  - Error is logged with stderr output.
  - That server is marked as not connected.

Suggestions for Future Enhancements:
- Retry auto-restart logic for failed servers.
- Per-tool or per-server timeouts (not just global).
- Better JSON-RPC schema validation on responses.
- Optional socket transport support.
- Logging interface (instead of just print).
- Server grouping or tag filters for better UX.
- Dependency resolution between tools (e.g., ThreatFox depends on AbuseIPDB).

File Location:
- Core MCP runtime — typically imported by `warden_server.py` or `ToolExecutor`.

====================================================================================

====================================================================================
llm_interface.py - LLM Integration for TheWarden via Ollama (Qwen3)
====================================================================================

Purpose:
- Provides a wrapper interface for interacting with the Qwen3 Large Language Model (LLM) via **Ollama's local REST API**.
- Enables secure, structured communication between TheWarden and the LLM for decision-making and final report generation.

What It Does:
- Formats security analysis context, tool usage history, and tool availability into **structured prompts**.
- Sends those prompts to the Qwen3 model (via `localhost:11434`) and receives structured JSON decisions or narrative reports.
- Parses the LLM's responses, extracting structured decisions like:
  - What tool to call next
  - Final report content
- Handles reasoning output wrapped in `<think>` tags and robust JSON parsing.

How Long Does It Take to Run?
- Internal calls (like `_call_llm()`) depend entirely on:
  - **Model size** (e.g., `qwen3:8b`)
  - **Hardware acceleration** (e.g., MPS/ROCm/GPU or pure CPU)
  - **Prompt length and tool result verbosity**
- On a typical modern CPU, single-shot generation takes **~3–10 seconds**, longer if multi-turn history or large tool results are included.

Importance to TheWarden System:
- Acts as the **decision-making brain** of the entire SOC analyst workflow:
  - Determines next steps in analysis sessions.
  - Evaluates previous tool results and chooses the optimal path forward.
  - Generates final professional-grade reports after investigations.

Core Functions:
1. `__init__()`:
   - Configures endpoint, model name, and system persona for TheWarden (SOC analyst identity).
   - Injects Zulu (UTC) timestamp into the persona message for context awareness.
2. `_call_llm(messages)`:
   - Low-level wrapper to invoke Ollama API with messages and model parameters.
   - Returns plain string response content or logs errors.
3. `get_next_action(analysis_context, available_tools)`:
   - Builds a SOC-style prompt from context (iteration count, prior tools, results).
   - Sends prompt to LLM and expects a **JSON object** with:
     - `action`: "use_tool" or "complete"
     - `reasoning`: Why it made the decision
     - `tool_name`, `arguments` (if action is use_tool)
   - Extracts and validates structured JSON, even if it's embedded in LLM noise or `<think>` tags.
4. `generate_final_analysis(analysis_context)`:
   - Builds a final-report prompt from all tool usage and results.
   - Instructs the LLM to return a professional multi-part SOC analysis report.

How It Integrates:
- Used by `ToolExecutor` or a central LLM orchestration loop inside `warden_server.py`.
- After each tool is executed, `get_next_action()` is called to decide whether to continue or conclude.
- When LLM says to `"complete"`, `generate_final_analysis()` is triggered to wrap up the session.
LLM Output Format (Expected):

{
  "action": "use_tool" | "complete",
  "reasoning": "...",
  "tool_name": "some_tool",
  "arguments": {"arg": "value"}
}

Failover Handling:
    - Gracefully logs and handles:
    - LLM errors (timeout, malformed JSON)
    - Unexpected or missing fields
    - Recovers from partial output via regex fallback extraction

Suggestions for Future Improvements:
- Use a streaming interface for faster UX in terminal apps.
- Add retry/backoff logic for transient LLM failures.
- Separate out prompt templates for easier tuning or multilingual use.
- Support multi-model fallback (e.g., if Qwen3 fails, try a local Mistral or Claude).
- Optional logging of all prompts/responses for auditing (currently not persisted).
- More rigorous JSON schema validation (e.g., with pydantic).
- Timeout reduction for quick tool assessments (~10s for small decisions).

Example Interaction Flow:
ToolExecutor calls get_next_action() with:
- "user_query" = "Is this IP suspicious?"
- "iteration" = 1
- Tool results = results from ThreatFox, AbuseIPDB
LLM returns:
{
  "action": "use_tool",
  "reasoning": "I want to cross-reference with Elastic logs...",
  "tool_name": "elastic.search_ip",
  "arguments": {"ip": "45.33.32.156"}
}

====================================================================================
EXAMPLE PIPELINE WORKFLOW

┌────────────┐
│ theWarden  │ ←─────────────── User sends query (e.g. suspicious IP)
└────┬───────┘
     │ calls
     ▼
┌──────────────┐
│ LLM Interface│ ←──── loads system prompt, handles token I/O
└────┬─────────┘
     │ sends/receives
     ▼
┌───────────-─┐       ┌───────────────┐
│ LLM (Qwen3) │ ⇄──▶  Tool Executor   │ ←──── Decision: which tools to call?
└────┬────────┘       └────┬──────────┘
     │                     │
     ▼                     ▼
FINAL REPORT    ┌───────────────────────┐
                │     MCP Manager       │ ←──── Parses config.json
                └────┬───────────┬──────┘
                     │           │
        ┌────────────▼─┐     ┌───▼────────────┐
        │ abuseipdb.py │     │ threatfox.py   │  (MCP Servers)
        └──────────────┘     └────────────────┘
               │                   │
               ▼                   ▼
        External APIs (e.g.       Threat Intelligence Sources
        AbuseIPDB, Elastic, etc.)

